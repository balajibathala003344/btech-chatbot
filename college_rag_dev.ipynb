{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7f8f47-f6d8-4578-8283-ff3a4b28004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6003738a-2931-4df8-9818-7eda2a3de484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyAHxV-uXIZ6mxe-RQFymVO73ju3f7v7-g4\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "\n",
    "print(os.getenv(\"GEMINI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8deed5-12bc-413b-9bac-277e6fcf4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDFs loaded: 10\n",
      "Academic_Regulations.pdf -> characters: 1573\n",
      "BTech_Academic_Regulations.pdf -> characters: 1131\n",
      "BTech_Branch_Wise_Syllabus.pdf -> characters: 692\n",
      "BTech_Placement_Policy.pdf -> characters: 433\n",
      "BTech_Project_and_Internship_Guidelines.pdf -> characters: 404\n",
      "BTech_Student_Handbook.pdf -> characters: 365\n",
      "Examination_Guidelines.pdf -> characters: 1058\n",
      "Placement_Policy.pdf -> characters: 1038\n",
      "syllabus.pdf -> characters: 452032\n",
      "TCS NQT IMP CONCEPTS.pdf -> characters: 601\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted = page.extract_text()\n",
    "        if extracted:\n",
    "            text += extracted + \"\\n\"\n",
    "    return text\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(DATA_DIR):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        text = load_pdf_text(os.path.join(DATA_DIR, file))\n",
    "        documents.append({\n",
    "            \"source\": file,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "print(\"Total PDFs loaded:\", len(documents))\n",
    "for doc in documents:\n",
    "    print(doc[\"source\"], \"-> characters:\", len(doc[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c2451-34e3-426c-8879-71066fcd3c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2805e8b8-0f33-4743-90fc-37ac59065e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1153\n",
      "\n",
      "Sample chunk:\n",
      "\n",
      "Academic Regulations â€“ Undergraduate Programs \n",
      "1. Introduction \n",
      "These academic regulations govern all undergraduate students enrolled in the institution. The \n",
      "objective is to maintain academic discipline, ensure fair evaluation, and promote holistic learning. \n",
      " \n",
      "2. Academic Year & Semester System \n",
      "â€¢ The academic year is divided into two semesters: \n",
      "o Odd Semester (Julyâ€“November) \n",
      "o Even Semester (Januaryâ€“May) \n",
      "â€¢ Each semester consists of: \n",
      "o Minimum 90 instructional days \n",
      "o Internal assessments \n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    small_chunks = chunk_text(doc[\"text\"])\n",
    "    for ch in small_chunks:\n",
    "        chunks.append({\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "print(\"Total chunks created:\", len(chunks))\n",
    "print(\"\\nSample chunk:\\n\")\n",
    "print(chunks[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbc2505-cf58-4cbf-8b11-ab734cd7b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf85060-b97a-44c7-927d-055127623454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created for chunks: 1153\n",
      "Vector size: 384\n"
     ]
    }
   ],
   "source": [
    "# Load embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Take only text from chunks\n",
    "texts = [chunk[\"text\"] for chunk in chunks]\n",
    "\n",
    "# Convert text to vectors\n",
    "embeddings = embedder.encode(texts)\n",
    "\n",
    "print(\"Embeddings created for chunks:\", len(embeddings))\n",
    "print(\"Vector size:\", embeddings.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ada652-1021-48a5-9982-f6230582e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks stored in FAISS: 1153\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to FAISS\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "print(\"Total chunks stored in FAISS:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6120a381-8562-4cc3-a19c-43c30638d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and chunks saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save FAISS index and chunks for Streamlit\n",
    "faiss.write_index(index, \"faiss.index\")\n",
    "np.save(\"chunks.npy\", chunks)\n",
    "\n",
    "print(\"FAISS index and chunks saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f023574-a750-4a92-b6e2-002db06b8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved chunks:\n",
      "\n",
      "Source: BTech_Academic_Regulations.pdf\n",
      " of an Odd Semester (Julyâ€“November) and an Even Semester (Januaryâ€“May).\n",
      "3. Credit System\n",
      "The program follows a credit-based system. One credit corresponds to one hour of theory per week or two\n",
      "hours of lab per week.\n",
      "Students must earn all prescribed credits to be eligible for graduation.\n",
      "4. Attendance Requirements\n",
      "A minimum of 75% attendance is mandatory in each subject.\n",
      "Students with attendance between 65% and 74% may be condoned upon payment of a fee.\n",
      "Students with attendance below 65% are not\n",
      "--------------------------------------------------\n",
      "Source: Academic_Regulations.pdf\n",
      "nts \n",
      "â€¢ Minimum 75% attendance is mandatory in each subject. \n",
      "â€¢ Students with attendance between 65%â€“74% may be condoned after paying a fee. \n",
      "â€¢ Students with attendance below 65% are not eligible to appear for examinations. \n",
      " \n",
      "5. Promotion Rules \n",
      "â€¢ Students are promoted semester-wise. \n",
      "â€¢ A student may carry a maximum of 2 backlogs to the next academic year. \n",
      "â€¢ Promotion to final year requires completion of at least 70% of total credits. \n",
      " \n",
      "6. Academic Integrity \n",
      "â€¢ Malpractice in exams or assignme\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Student question\n",
    "query = \"What is the minimum attendance required?\"\n",
    "\n",
    "# Convert question to vector\n",
    "query_embedding = embedder.encode([query])\n",
    "\n",
    "# Search FAISS for top 2 relevant chunks\n",
    "D, I = index.search(np.array(query_embedding), k=2)\n",
    "\n",
    "print(\"Retrieved chunks:\\n\")\n",
    "\n",
    "for idx in I[0]:\n",
    "    print(\"Source:\", chunks[idx][\"source\"])\n",
    "    print(chunks[idx][\"text\"])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae60c107-1cce-49fb-9f79-cb64b52cbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82599789-a1ec-46ef-82da-8d6b97aa7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gemini client using API key from environment variable\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Combine retrieved chunks into context\n",
    "context = \"\\n\\n\".join([chunks[idx][\"text\"] for idx in I[0]])\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a college assistant chatbot.\n",
    "\n",
    "Answer the question ONLY using the context below.\n",
    "If the answer is not present, say \"Information not found in college documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e3a6a9-0426-42d1-82d7-2e2c5d3ecd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A minimum of 75% attendance is mandatory in each subject.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e8f307c-196f-487c-aafd-c0d5009143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ“ College Assistant Chatbot\n",
      "Type 'exit' to stop\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  ipl 2024 winner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot: Information not found in college documents.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Goodbye ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ“ College Assistant Chatbot\")\n",
    "print(\"Type 'exit' to stop\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    \n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye ðŸ‘‹\")\n",
    "        break\n",
    "\n",
    "    # Convert query to vector\n",
    "    query_embedding = embedder.encode([query])\n",
    "\n",
    "    # Retrieve top 2 chunks\n",
    "    D, I = index.search(np.array(query_embedding), k=2)\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([chunks[idx][\"text\"] for idx in I[0]])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a college assistant chatbot.\n",
    "\n",
    "    Answer the question ONLY using the context below.\n",
    "    If the answer is not present, say \"Information not found in college documents.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "\n",
    "    print(\"\\nChatbot:\", response.text)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067ceb5-c82a-4626-9925-ba0dc2ca1a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
